{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0e1fafd-0085-4605-85dd-52a5fe90c331",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_label_set = ['O','CORP', 'CW', 'GRP', 'LOC', 'PER', 'PROD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82a71262-0917-4a98-abea-d3de6603767f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782fa7f1-773a-4cca-b689-aadb525efc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def read_conll_file(file_path):\n",
    "    sentences = []\n",
    "    ner_tags = []\n",
    "    tokens = []\n",
    "    tags = []\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "\n",
    "            if not line:\n",
    "                if tokens:\n",
    "                    sentences.append(tokens)\n",
    "                    ner_tags.append(tags)\n",
    "                    tokens = []\n",
    "                    tags = []\n",
    "                continue\n",
    "\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "\n",
    "            parts = line.split()\n",
    "            if len(parts) >= 4:\n",
    "                token, _, _, tag = parts\n",
    "                tokens.append(token)\n",
    "                tags.append(tag)\n",
    "\n",
    "    if tokens:\n",
    "        sentences.append(tokens)\n",
    "        ner_tags.append(tags)\n",
    "\n",
    "    return sentences, ner_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de7e0c8-17d7-42e0-bcd8-6bb544251061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_conll_dataset_from_dir(data_dir):\n",
    "    all_sentences = []\n",
    "    all_tags = []\n",
    "\n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.endswith(\".conll\"):\n",
    "            path = os.path.join(data_dir, filename)\n",
    "            sents, tags = read_conll_file(path)\n",
    "            all_sentences.extend(sents)\n",
    "            all_tags.extend(tags)\n",
    "\n",
    "    return all_sentences, all_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea5b4f40-b8d5-43ba-bd72-dbd7fde78e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def create_hf_dataset(sentences, tags):\n",
    "    data = [{\"tokens\": s, \"ner_tags\": t} for s, t in zip(sentences, tags)]\n",
    "    return Dataset.from_list(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f7a08d3-4f9c-4939-8af9-1b06bc2fe2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from transformers import XLMRobertaTokenizerFast, XLMRobertaModel\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d87ac2c-2fa6-48d8-a73a-63404d67a1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['his', 'playlist', 'includes', 'sonny', 'sharrock', ',', 'gza', ',', 'country', 'teasers', 'and', 'the', 'notorious', 'b.i.g.'], 'ner_tags': ['O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-GRP', 'I-GRP', 'O', 'B-PER', 'I-PER', 'I-PER']}\n"
     ]
    }
   ],
   "source": [
    "train_data_dir = \"multiconer2022/EN-English/train\"\n",
    "test_data_dir= \"multiconer2022/EN-English/test\"\n",
    "val_data_dir= \"multiconer2022/EN-English/val\"\n",
    "\n",
    "sentences, ner_tags = load_conll_dataset_from_dir(train_data_dir)\n",
    "train_dataset = create_hf_dataset(sentences, ner_tags)\n",
    "#test\n",
    "sentences1, ner_tags1 = load_conll_dataset_from_dir(test_data_dir)\n",
    "test_dataset = create_hf_dataset(sentences1, ner_tags1)\n",
    "#val\n",
    "sentences, ner_tags = load_conll_dataset_from_dir(val_data_dir)\n",
    "validation_dataset = create_hf_dataset(sentences, ner_tags)\n",
    "\n",
    "print(train_dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc41fd55-5133-4aeb-a3b9-e5fad49409bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['ford', 'cargo', 'price'], 'ner_tags': ['B-PROD', 'I-PROD', 'O']}\n",
      "{'tokens': ['ease', 'on', 'down', 'the', 'road', 'â€”', 'charlie', 'smalls', '(', 'diana', 'ross', 'and', 'michael', 'jackson', 'in', 'the', 'wiz', ')'], 'ner_tags': ['B-CW', 'I-CW', 'I-CW', 'I-CW', 'I-CW', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-CW', 'I-CW', 'O']}\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset[0])\n",
    "print(validation_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e466dd-2fe1-4d0f-be66-e6cd1a2c15d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "train_indices = random.sample(range(len(train_dataset)), 100)\n",
    "train_subset = Subset(train_dataset, train_indices)\n",
    "\n",
    "tokenizer = XLMRobertaTokenizerFast.from_pretrained('xlm-roberta-large')\n",
    "encoder = XLMRobertaModel.from_pretrained('xlm-roberta-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d85fd77-89ec-4865-a29d-937ac009c343",
   "metadata": {},
   "outputs": [],
   "source": [
    "span_label_set = ['B', 'I', 'O']\n",
    "span2id = {label: idx for idx, label in enumerate(span_label_set)}\n",
    "entity2id = {label: idx for idx, label in enumerate(entity_label_set)}\n",
    "\n",
    "def split_ner_tags(ner_tags):\n",
    "    span_labels = []\n",
    "    entity_labels = []\n",
    "    for tag in ner_tags:\n",
    "        if tag == \"O\":\n",
    "            span_labels.append(\"O\")\n",
    "            entity_labels.append(\"O\")\n",
    "        else:\n",
    "            bio, entity = tag.split(\"-\", 1)\n",
    "            span_labels.append(bio)\n",
    "            entity_labels.append(entity)\n",
    "    return span_labels, entity_labels\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch_tokens = [item['tokens'] for item in batch]\n",
    "    batch_ner_tags = [item['ner_tags'] for item in batch]\n",
    "\n",
    "    batch_span_labels = []\n",
    "    batch_entity_labels = []\n",
    "    for ner_tags in batch_ner_tags:\n",
    "        span_labels, entity_labels = split_ner_tags(ner_tags)\n",
    "        batch_span_labels.append([span2id[label] for label in span_labels])\n",
    "        batch_entity_labels.append([entity2id[label] for label in entity_labels])\n",
    "\n",
    "    encodings = tokenizer(batch_tokens, is_split_into_words=True, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "    input_ids = encodings['input_ids']\n",
    "    attention_mask = encodings['attention_mask']\n",
    "\n",
    "    max_len = input_ids.size(1)\n",
    "    padded_span_labels = []\n",
    "    padded_entity_labels = []\n",
    "    for idx, (span_labels, entity_labels) in enumerate(zip(batch_span_labels, batch_entity_labels)):\n",
    "        word_ids = encodings.word_ids(batch_index=idx)\n",
    "        aligned_span = []\n",
    "        aligned_entity = []\n",
    "        prev_word_id = None\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                aligned_span.append(-100)\n",
    "                aligned_entity.append(-100)\n",
    "            elif word_id != prev_word_id:\n",
    "                aligned_span.append(span_labels[word_id])\n",
    "                aligned_entity.append(entity_labels[word_id])\n",
    "            else:\n",
    "                aligned_span.append(-100)\n",
    "                aligned_entity.append(-100)\n",
    "            prev_word_id = word_id\n",
    "        padded_span_labels.append(aligned_span)\n",
    "        padded_entity_labels.append(aligned_entity)\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'span_labels': torch.tensor(padded_span_labels),\n",
    "        'entity_labels': torch.tensor(padded_entity_labels)\n",
    "    }\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65ee3801-1775-40d3-9435-6a9a7a301322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "class E2DAEndogenous(nn.Module):\n",
    "    def __init__(self, hidden_dim=1024, num_span_labels=len(span_label_set), num_entity_labels=len(entity_label_set)):\n",
    "        super(E2DAEndogenous, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.shared_extractor = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.span_extractor = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.type_extractor = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.span_head = nn.Linear(hidden_dim, num_span_labels)\n",
    "        self.type_head = nn.Linear(hidden_dim, num_entity_labels)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.encoder(input_ids, attention_mask=attention_mask)\n",
    "        H = outputs.last_hidden_state\n",
    "\n",
    "        H_share = self.shared_extractor(H)\n",
    "        H_st = H - H_share\n",
    "        H_span = self.span_extractor(H_st)\n",
    "        H_type = self.type_extractor(H_st)\n",
    "        H_sp = H_span + H_share\n",
    "        H_tp = H_type + H_share\n",
    "\n",
    "        span_logits = self.span_head(self.dropout(H_sp))\n",
    "        type_logits = self.type_head(self.dropout(H_tp))\n",
    "        return span_logits, type_logits, H_span, H_type, H_share\n",
    "\n",
    "# Endogenous Augmentation Loss\n",
    "def compute_covariance_matrix(features, labels, num_classes):\n",
    "    batch_size, seq_len, dim = features.size()\n",
    "    device = features.device  # Get the device of the input features\n",
    "    features_flat = features.view(-1, dim)\n",
    "    labels_flat = labels.view(-1)\n",
    "    cov_matrices = []\n",
    "    for c in range(num_classes):\n",
    "        class_features = features_flat[labels_flat == c]\n",
    "        if class_features.numel() > 0 and class_features.size(0) > 1:\n",
    "            cov = torch.cov(class_features.T)\n",
    "            cov_matrices.append(cov + torch.eye(dim, device=device) * 1e-6)\n",
    "        else:\n",
    "            cov_matrices.append(torch.eye(dim, device=device) * 1e-6)\n",
    "    return torch.stack(cov_matrices)\n",
    "\n",
    "def endogenous_loss(logits, labels, features, head_weights, head_bias, lambda_):\n",
    "    batch_size, seq_len, num_classes = logits.size()\n",
    "    cov_matrices = compute_covariance_matrix(features, labels, num_classes)\n",
    "\n",
    "    loss = 0\n",
    "    valid_tokens = 0\n",
    "    for i in range(batch_size):\n",
    "        for j in range(seq_len):\n",
    "            if labels[i, j] != -100:\n",
    "                c_i = labels[i, j]\n",
    "                h_i = features[i, j]\n",
    "                log_sum_exp = 0\n",
    "                for c_j in range(num_classes):\n",
    "                    if c_j != c_i:\n",
    "                        delta_w = head_weights[c_j] - head_weights[c_i]\n",
    "                        delta_b = head_bias[c_j] - head_bias[c_i]\n",
    "                        mean_term = delta_w @ h_i + delta_b\n",
    "                        var_term = (lambda_ / 2) * delta_w @ cov_matrices[c_i] @ delta_w\n",
    "                        log_sum_exp += torch.exp(mean_term + var_term)\n",
    "                loss += torch.log(1 + log_sum_exp)\n",
    "                valid_tokens += 1\n",
    "    return loss / valid_tokens if valid_tokens > 0 else torch.tensor(0.0, device=logits.device)\n",
    "\n",
    "# Orthogonality Loss\n",
    "def orthogonality_loss(H_span, H_type, H_share):\n",
    "    dot1 = (H_span * H_share).sum(dim=-1).pow(2).mean()\n",
    "    dot2 = (H_type * H_share).sum(dim=-1).pow(2).mean()\n",
    "    dot3 = (H_span * H_type).sum(dim=-1).pow(2).mean()\n",
    "    return dot1 + dot2 + dot3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1faf1d4f-afb9-448d-aadf-6701ef965c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "E2DAEndogenous(\n",
       "  (encoder): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): XLMRobertaPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (shared_extractor): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (span_extractor): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (type_extractor): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (span_head): Linear(in_features=1024, out_features=3, bias=True)\n",
       "  (type_head): Linear(in_features=1024, out_features=7, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = E2DAEndogenous().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "model.load_state_dict(torch.load(\"best_e2da_endogenousV1.pth\"))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859810ee-a65e-43e4-ac0d-4439367c541c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/440, Training Loss: 0.3423\n",
      "Model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/440, Training Loss: 0.1942\n",
      "Model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/440, Training Loss: 0.1367\n",
      "Model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/440, Training Loss: 0.1159\n",
      "Model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/440, Training Loss: 0.1239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/440, Training Loss: 0.1102\n",
      "Model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/440, Training Loss: 0.1180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/440, Training Loss: 0.0967\n",
      "Model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/440, Training Loss: 0.1026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/440, Training Loss: 0.0981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/440, Training Loss: 0.0947\n",
      "Model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/440, Training Loss: 0.0964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/440, Training Loss: 0.0934\n",
      "Model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/440, Training Loss: 0.0836\n",
      "Model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/440, Training Loss: 0.0861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/440, Training Loss: 0.0895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/440, Training Loss: 0.0874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/440, Training Loss: 0.0829\n",
      "Model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/440, Training Loss: 0.0827\n",
      "Model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/440, Training Loss: 0.0943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/440, Training Loss: 0.0794\n",
      "Model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/440, Training Loss: 0.0863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/440, Training Loss: 0.0780\n",
      "Model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/440, Training Loss: 0.0833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/440, Training Loss: 0.0762\n",
      "Model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/440, Training Loss: 0.0729\n",
      "Model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/440, Training Loss: 0.0760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/440, Training Loss: 0.0835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/440, Training Loss: 0.0792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/440, Training Loss: 0.0727\n",
      "Model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/440, Training Loss: 0.0674\n",
      "Model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/440, Training Loss: 0.0715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/440, Training Loss: 0.0728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/440, Training Loss: 0.0758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/440, Training Loss: 0.0745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/440, Training Loss: 0.0599\n",
      "Model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/440, Training Loss: 0.0645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/440, Training Loss: 0.0676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/440, Training Loss: 0.0643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/440, Training Loss: 0.0601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/440, Training Loss: 0.0613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/440, Training Loss: 0.0616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/440, Training Loss: 0.0614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/440, Training Loss: 0.0733\n",
      "Early stopping triggered. Stopping training.\n",
      "Training complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "alpha = 0.01\n",
    "max_epochs = 440\n",
    "patience = 8\n",
    "best_train_loss = np.inf\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{max_epochs}\", leave=False)\n",
    "    \n",
    "    for batch in train_loader_tqdm:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        span_labels = batch['span_labels'].to(device)\n",
    "        entity_labels = batch['entity_labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        span_logits, type_logits, H_span, H_type, H_share = model(input_ids, attention_mask)\n",
    "\n",
    "        span_weights = model.span_head.weight.detach()\n",
    "        span_bias = model.span_head.bias.detach()\n",
    "        type_weights = model.type_head.weight.detach()\n",
    "        type_bias = model.type_head.bias.detach()\n",
    "\n",
    "        lambda_ = 1.5 * (epoch + 1) / max_epochs\n",
    "        span_loss = endogenous_loss(span_logits, span_labels, H_span, span_weights, span_bias, lambda_)\n",
    "        type_loss = endogenous_loss(type_logits, entity_labels, H_type, type_weights, type_bias, lambda_)\n",
    "        ortho_loss = orthogonality_loss(H_span, H_type, H_share)\n",
    "\n",
    "        loss = span_loss + type_loss + alpha * ortho_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        train_loader_tqdm.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{max_epochs}, Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    if avg_train_loss < best_train_loss:\n",
    "        best_train_loss = avg_train_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), \"best_e2da_endogenousV1.pth\")\n",
    "        print(\"Model saved!\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(\"Early stopping triggered. Stopping training.\")\n",
    "        break\n",
    "\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402b7044-c490-4eff-bb2d-940edc9937b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save the model after training\n",
    "torch.save(model.state_dict(), \"e2da_endogenous3v1.pth\")\n",
    "print(\"Model saved successfully.\")\n",
    "# model.load_state_dict(torch.load(\"e2da_endogenous.pth\"))\n",
    "# model.to(device)\n",
    "# model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dee3883-0ca1-4881-8949-eb3fbaaf4daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Micro-F1 (non-'O'): 0.6253\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "model.eval()\n",
    "test_preds, test_labels = [],[]\n",
    "\n",
    "test_loader_tqdm = tqdm(test_loader, desc=\"Testing\", leave=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader_tqdm:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        span_labels = batch['span_labels'].to(device)\n",
    "        entity_labels = batch['entity_labels'].to(device)\n",
    "\n",
    "        span_logits, type_logits, _, _, _ = model(input_ids, attention_mask)\n",
    "        span_preds = torch.argmax(span_logits, dim=-1)\n",
    "        type_preds = torch.argmax(type_logits, dim=-1)\n",
    "\n",
    "        for i in range(span_preds.size(0)):\n",
    "            for j in range(span_preds.size(1)):\n",
    "                if span_labels[i, j] != -100:\n",
    "                    pred_label = f\"{span_label_set[span_preds[i, j]]}-{entity_label_set[type_preds[i, j]]}\" if span_preds[i, j] != 2 else \"O\"\n",
    "                    true_label = f\"{span_label_set[span_labels[i, j]]}-{entity_label_set[entity_labels[i, j]]}\" if span_labels[i, j] != 2 else \"O\"\n",
    "\n",
    "                    # Only append non-\"O\" labels to the lists\n",
    "                    if pred_label != \"O\" and true_label != \"O\":\n",
    "                        test_preds.append(pred_label)\n",
    "                        test_labels.append(true_label)\n",
    "\n",
    "# Compute Micro-F1 Score for non-\"O\" predictions\n",
    "micro_f1 = f1_score(test_labels, test_preds, average='micro')\n",
    "print(f\"Test Micro-F1 (non-'O'): {micro_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fd17bb-4103-4534-b3f9-c2d58da74833",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
